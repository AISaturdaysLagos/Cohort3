{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI6Lagos DL week5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "AEyl0-_1kmnZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEfgzk27sf4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Week 5 : Introduction to Convolutional Neural Networks, Kaggle Competitions and Computer Vision Techniques"
      ]
    },
    {
      "metadata": {
        "id": "V701kpcMkmM2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "0qp6lNtEzbKU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Recap of Neural Networks\n",
        "\n",
        "In this notebook, we examine the representation of neural networks and how they learn. We will build a neural network from scratch with just numpy, implementing both the forward and backward propagation. Our implementation will then be used to build and train a neural network model using the MNIST dataset.\n",
        "\n",
        "Tensorflow will be intoduced in this notebook and will also be used to train a neural network on the MNIST dataset. We will check how well the model built with our implementation performs compared to that built with tensorflow."
      ]
    },
    {
      "metadata": {
        "id": "i3FwCT6sz5wE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural networks are represented by layers of neurons. There are three types of layers, namely;\n",
        "\n",
        "- The input layer, \n",
        "- The hidden layers, \n",
        "- The output layer.\n",
        "\n",
        "\n",
        "A network can have more than one hidden layers but can only have one input layer and one output layer. Each layer has a Weight matrix and bias vector. These are the network's parameters.  At the output layer, there is usually a loss function which serves as a check of model performace.\n",
        "\n",
        "The figure below shows how a neural network is represented.\n",
        "![alt_text](http://i67.tinypic.com/dlj42e.png\" border=\"0\" alt=\"Image and video hosting by TinyPic\"></a>\"CNN Architecture for MNIST Classification\")"
      ]
    },
    {
      "metadata": {
        "id": "49W5U1qFC86h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the implementation that follows, a layer would be treated as an object and all layer parameters will become attributes of the layer object."
      ]
    },
    {
      "metadata": {
        "id": "ZKu0Yw2KtQre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## I don't want warnings at all!!\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "89sqwgNvEGYo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "import numpy. For this part, we will use just numpy no framework is allowed. just numpy !"
      ]
    },
    {
      "metadata": {
        "id": "xr8epyQ1ppdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pdb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nzltbCBnEkzB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A Layer class to serve as the base class for all layer types."
      ]
    },
    {
      "metadata": {
        "id": "41zY-Ui-EjLR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To build layers\n",
        "class Layer(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.init = False\n",
        "    self.learnable_params = False\n",
        "  \n",
        "  def forward(self, x):\n",
        "    pass\n",
        "  \n",
        "  def backward(self, d):\n",
        "    pass\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4n0QJDY8E2L0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Losses are usually calculated at the output layer. There are several loss functions. The class below serves as the base class for all loss functions."
      ]
    },
    {
      "metadata": {
        "id": "Sq9Nx0TazYUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# For loss functions  \n",
        "class Loss(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def forward(self, z, y):\n",
        "    pass\n",
        "  \n",
        "  def backward(self):\n",
        "    pass\n",
        "  \n",
        "  def __call__(self, z, y):\n",
        "    return self.forward(z, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-mQzoIqFNgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dense layers are used in vanilla neural networks. The layers in the figure above are all dense layers. They are called dense layers because for a layer, each neuron has a connection to all the neurons in the preceding layer."
      ]
    },
    {
      "metadata": {
        "id": "R156UEeQF6bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Dense(Layer):\n",
        "  \n",
        "  def __init__(self, units):\n",
        "    super(Dense, self).__init__()\n",
        "    self.nout = units # number of output units\n",
        "    self.learnable_params = True\n",
        "  \n",
        "  def forward(self, x):\n",
        "    self.nin = x.shape[0] # number of input units\n",
        "    self.x = x # save the input to the layer\n",
        "    \n",
        "    \n",
        "    # initialize weights only once\n",
        "    if(not self.init):\n",
        "      self.init = True\n",
        "      self.W = np.random.normal(0, np.sqrt((2/(self.nin+self.nout))), (self.nout, self.nin)) # using xavier initialization\n",
        "      self.b = np.zeros(self.nout).reshape(-1,1)\n",
        "    \n",
        "    # weighted sum\n",
        "    z = self.W.dot(x) + self.b\n",
        "    \n",
        "    return z\n",
        "  \n",
        "  def backward(self, din):\n",
        "    \n",
        "    self.backprop = True\n",
        "    \n",
        "    #pdb.set_trace()\n",
        "    self.dx = self.W.T.dot(din)\n",
        "    self.dW = din.dot(self.x.T)\n",
        "    self.db = din.sum(axis=1).reshape((-1,1))\n",
        "    \n",
        "    return self.dx\n",
        "  \n",
        "  def update(self, lr):\n",
        "    \n",
        "    if(self.backprop):\n",
        "      self.W -= lr*self.dW\n",
        "      self.b -= lr*self.db\n",
        "    \n",
        "    self.backprop = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWyFu6HWF-mP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Activation functions can be seen as layers that just applies a function to its input. Here, we used the ReLU activation function. Other activation functions can also be implemented in this manner."
      ]
    },
    {
      "metadata": {
        "id": "G73AFBVX0HU2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Relu(Layer):\n",
        "  \n",
        "  def __init(self):\n",
        "    super(Relu, self).__init__()\n",
        "    \n",
        "    \n",
        "  def forward(self, x):\n",
        "    self.x = x # save the layer's input\n",
        "    a = np.maximum(x, 0) # relu function\n",
        "    return a\n",
        "  \n",
        "  def backward(self, da):\n",
        "    dx = ((self.x > 0)*1)*da\n",
        "    \n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ROcBuxCaGUV4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loss functions are very important in Neural Networks and to some extent, determine how the network learns. We will implement the softmax cross entropy loss. This implementation might look suspicious. Don't worry. Somethings have be included for numerical stability."
      ]
    },
    {
      "metadata": {
        "id": "nlMqDdR4GuS_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For the softmax loss\n",
        "class SoftmaxCrossEntropy(Loss):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(SoftmaxCrossEntropy, self).__init__()\n",
        "    \n",
        "  def forward(self, z, y):\n",
        "    self.logits = z # weighted sum (output layer)\n",
        "    self.labels = y # true labels\n",
        "    \n",
        "\n",
        "    \n",
        "    c = np.max(self.logits, axis=0).reshape((1,-1)) # get the maximum weighted sum\n",
        "    scaled_z = self.logits - c # subtract the maximum from all\n",
        "    \n",
        "    logit_exp = np.exp(scaled_z) # e^z\n",
        "    sum_exp = np.sum(logit_exp, axis=0).reshape((1,-1)) # sum(e^z)\n",
        "    \n",
        "    norm_probs = logit_exp / sum_exp # e^z/sum(e^z) = normalized probabilities\n",
        "    \n",
        "    self.t_logit = np.sum(self.labels*self.logits, axis=0).reshape((1,-1)) # the logit of the true class\n",
        "    \n",
        "    logsumexp = np.log(sum_exp) # to be used for logsumexp algorithm for softmax loss\n",
        "\n",
        "\n",
        "    loss = -1*(self.t_logit - logsumexp - c) # logsumexp for loss\n",
        "    \n",
        "    self.probs = norm_probs # normalized probabilies saved\n",
        "    \n",
        "    return loss\n",
        "  \n",
        "  def backward(self):\n",
        "    return self.probs - self.labels\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xXQsOpSWGteZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, since everything needed to build the neural network is in place. We can now go ahead and build the Network."
      ]
    },
    {
      "metadata": {
        "id": "IfO1da6F1Fri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(object):\n",
        "  \n",
        "  def __init__(self, representation):\n",
        "    \n",
        "    self.l = representation\n",
        "    self.layers = []\n",
        "    self.loss = 0\n",
        "    self.step = 0\n",
        "    \n",
        "    for l in range(1, len(self.l)-1):\n",
        "      dense = Dense(self.l[l])\n",
        "      activation = Relu()\n",
        "      \n",
        "      self.layers += [dense, activation]\n",
        "    \n",
        "    # output layer\n",
        "    dense = Dense(self.l[-1])\n",
        "    self.layers += [dense]\n",
        "    \n",
        "  \n",
        "  # forward propagation\n",
        "  def forward(self, x):\n",
        "    \n",
        "    for l in range(len(self.layers)):\n",
        "      x = self.layers[l](x)\n",
        " \n",
        "    self.z = x\n",
        "    return x\n",
        "  \n",
        "  # backward propagation\n",
        "  def backward(self, y):\n",
        "    lossfunction = SoftmaxCrossEntropy() # loss function\n",
        "    loss = lossfunction(self.z, y)\n",
        "    dloss = lossfunction.backward()\n",
        "    \n",
        "    # go through all the layers starting from the output layer\n",
        "    for l in range(1,len(self.layers)+1):\n",
        "      #pdb.set_trace()\n",
        "      dloss = self.layers[-l].backward(dloss)\n",
        "      \n",
        "    return loss\n",
        "  \n",
        "  def fit_one_step(self, x, y, BS, lr):\n",
        "    self.forward(x)\n",
        "    loss = self.backward(y)\n",
        "    self.loss += np.sum(loss)\n",
        "    self.step += BS\n",
        "    # update layer weight\n",
        "    for layer in self.layers:\n",
        "      if layer.learnable_params:\n",
        "        layer.update(lr)\n",
        "        \n",
        "        \n",
        "  def epoch_loss(self):\n",
        "    loss = self.loss/self.step\n",
        "    self.loss = 0\n",
        "    self.step = 0\n",
        "    return loss\n",
        "  \n",
        "  def predict(self, x):\n",
        "    z = self.forward(x)\n",
        "    loss = SoftmaxCrossEntropy()\n",
        "    loss.forward(z, np.zeros(z.shape))\n",
        "    \n",
        "    return loss.probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "82LlQDNaJ8_H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load  the MNIST dataset and train the model"
      ]
    },
    {
      "metadata": {
        "id": "bWbdaH1IKF-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoEwDY_qdHeZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist # get mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() # extract train and test sets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Qq9u5vHp6uo",
        "colab_type": "code",
        "outputId": "4e31fb3a-5047-42fd-ff44-a8df1994bbf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "n_train = train_images.shape[0]\n",
        "n_test = test_images.shape[0]\n",
        "image_size = train_images.shape[1:]\n",
        "\n",
        "print('There are {} train examples'.format(n_train))\n",
        "print('There are {} test example'.format(n_test))\n",
        "print('images are of size {}'.format(image_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 60000 train examples\n",
            "There are 10000 test example\n",
            "images are of size (28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_EfnNrNGMSsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8h6XiTQxMTAt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets display a random train example"
      ]
    },
    {
      "metadata": {
        "id": "gCFkh-AKqLb9",
        "colab_type": "code",
        "outputId": "a46dc6e7-a600-47dd-a034-cb5e2e762aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_images[0], cmap='gray')\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vKpvtkd2MYyV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets normalize the data.\n"
      ]
    },
    {
      "metadata": {
        "id": "vkCC7ZciN9U8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NSB2ATD4qMnF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images / train_images.max()\n",
        "test_images = test_images / train_images.max()\n",
        "\n",
        "# reshape\n",
        "no_inputs = np.prod(image_size)\n",
        "\n",
        "train_images = train_images.reshape((-1, no_inputs))\n",
        "test_images = test_images.reshape((-1, no_inputs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-NuVJUXwMEhe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "myNN = NeuralNetwork([no_inputs, 20,10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVgJQ2NSM5F6",
        "colab_type": "code",
        "outputId": "14fd69ce-a3ba-4a75-d78b-21ea869235ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# train model\n",
        "BS = 32\n",
        "epochs = 10\n",
        "no_steps = n_train//BS\n",
        "for ii in range(epochs):\n",
        "  for i in range(no_steps):\n",
        "    x = train_images[i*BS:i*BS+BS]\n",
        "\n",
        "    y = np.eye(10)[train_labels[i*BS:i*BS+BS]]\n",
        "    \n",
        "    myNN.fit_one_step(x.T,y.T,BS, 0.001)\n",
        "  \n",
        "  acc = (myNN.predict(train_images.T).argmax(axis=0) == np.int64(train_labels)).sum()/n_train\n",
        "  testacc = (myNN.predict(test_images.T).argmax(axis=0) == np.int64(test_labels)).sum()/n_test\n",
        "  print('epoch {}=> loss={} | train_accuracy: {} ; test_accuracy: {}'.format(ii,myNN.epoch_loss(), acc, testacc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0=> loss=0.4723468768962434 | train_accuracy: 0.9093166666666667 ; test_accuracy: 0.9136\n",
            "epoch 1=> loss=0.2687444523246783 | train_accuracy: 0.9243833333333333 ; test_accuracy: 0.9206\n",
            "epoch 2=> loss=0.23418273868951398 | train_accuracy: 0.9322333333333334 ; test_accuracy: 0.9231\n",
            "epoch 3=> loss=0.2116436811726788 | train_accuracy: 0.9376 ; test_accuracy: 0.9244\n",
            "epoch 4=> loss=0.19425727188302142 | train_accuracy: 0.9425666666666667 ; test_accuracy: 0.927\n",
            "epoch 5=> loss=0.18062778524174725 | train_accuracy: 0.9459 ; test_accuracy: 0.9309\n",
            "epoch 6=> loss=0.16940058642638622 | train_accuracy: 0.9488333333333333 ; test_accuracy: 0.933\n",
            "epoch 7=> loss=0.1599341889067568 | train_accuracy: 0.9508166666666666 ; test_accuracy: 0.9356\n",
            "epoch 8=> loss=0.15177655557424138 | train_accuracy: 0.9533 ; test_accuracy: 0.9374\n",
            "epoch 9=> loss=0.14451933012228269 | train_accuracy: 0.9551666666666667 ; test_accuracy: 0.9393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aUHXjFlCbgTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UJYc0g4Vd-iL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build model with tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "3gO27gSrkMba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QvLBmTdeeDEV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    \n",
        "    self.h1 = tf.keras.layers.Dense(20, activation='relu') # dense layer with relu activation\n",
        "    self.h2 = tf.keras.layers.Dense(10, activation='softmax') # dense layer with softmax activation\n",
        "    \n",
        "    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    self.optimizer = tf.keras.optimizers.SGD()\n",
        "    \n",
        "    self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "    self.test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "    self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "  def call(self, x):\n",
        "    \n",
        "    x = self.h1(x)\n",
        "    return self.h2(x)\n",
        "  \n",
        "  \n",
        "  \n",
        "  @tf.function\n",
        "  def train_step(self, x, label):\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(x)\n",
        "      loss = self.loss_object(label, predictions)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.train_loss(loss)\n",
        "    self.train_accuracy(label, predictions)\n",
        "    \n",
        "  @tf.function\n",
        "  def test_step(self, x, label):\n",
        "    predictions = model(x)\n",
        "    t_loss = self.loss_object(label, predictions)\n",
        "\n",
        "    self.test_loss(t_loss)\n",
        "    self.test_accuracy(label, predictions)\n",
        "    \n",
        "  def fit(self, x, y, x_t, y_t, epochs, BS):\n",
        "    \n",
        "    n_tr = x.shape[0]\n",
        "    n_te = x_t.shape[0]\n",
        "    train_steps = n_tr//BS\n",
        "    test_steps = n_te//BS\n",
        "    for i in range(epochs):\n",
        "      for step in range(train_steps):\n",
        "        X = x[step*BS:step*BS+BS]\n",
        "        Y = y[step*BS:step*BS+BS]\n",
        "        self.train_step(X, Y)\n",
        "\n",
        "      for step in range(test_steps):\n",
        "        X = x_t[step*BS:step*BS+BS]\n",
        "        Y = y_t[step*BS:step*BS+BS]\n",
        "        self.test_step(X, Y)\n",
        "\n",
        "      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "      print('epoch {}=> loss={} | train_accuracy: {} ; test_accuracy: {}'.format(i, self.train_loss.result(), self.train_accuracy.result(), self.test_accuracy.result()))\n",
        "\n",
        "model = MyModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGFhQPEMj0xi",
        "colab_type": "code",
        "outputId": "4698892f-acb3-4e3e-aac2-99b5a141956a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, test_images, test_labels, 5, 32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0=> loss=0.5933747887611389 | train_accuracy: 0.8442218899726868 ; test_accuracy: 0.8657163977622986\n",
            "epoch 1=> loss=0.5790340304374695 | train_accuracy: 0.8475950956344604 ; test_accuracy: 0.8677413463592529\n",
            "epoch 2=> loss=0.5660075545310974 | train_accuracy: 0.8506657481193542 ; test_accuracy: 0.8695857524871826\n",
            "epoch 3=> loss=0.5541143417358398 | train_accuracy: 0.853471040725708 ; test_accuracy: 0.8712729811668396\n",
            "epoch 4=> loss=0.543201744556427 | train_accuracy: 0.8560575246810913 ; test_accuracy: 0.8728265166282654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5VWDjCPamiWX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "using easier keras api"
      ]
    },
    {
      "metadata": {
        "id": "G7vK2LQmk31O",
        "colab_type": "code",
        "outputId": "40614082-e298-4c40-d9fe-d7a6eb5acd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        " \n",
        "  tf.keras.layers.Dense(20,input_shape=(no_inputs,), activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 1.7861 - accuracy: 0.4612\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 1.0986 - accuracy: 0.7450\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 0.8103 - accuracy: 0.8048\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 0.6711 - accuracy: 0.8324\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 0.5888 - accuracy: 0.8498\n",
            "10000/10000 [==============================] - 0s 41us/sample - loss: 39.1780 - accuracy: 0.8641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[39.17797664174521, 0.8641]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "iPpq_wO5mpn1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Digit Recognizer Kaggle Competition\n",
        "\n",
        "\n",
        "<a href=\"https://www.kaggle.com/c/digit-recognizer\" target=\"_blank\"><img src=\"http://i68.tinypic.com/23lh0zp.png\" border=\"0\" alt=\"Image and video hosting by TinyPic\"></a>"
      ]
    },
    {
      "metadata": {
        "id": "xZuleYiRleOz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro to CNNs\n",
        "\n",
        "<a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf\" target=\"_blank\"><img src=\"http://i68.tinypic.com/24mhshf.png\" border=\"0\" alt=\"Image and video hosting by TinyPic\"></a>"
      ]
    },
    {
      "metadata": {
        "id": "qk3LyUaRlqF7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}